{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680278e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c905c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel:\n",
    "    def __init__(self, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Vocabulaire : mapping entre mots et indices\n",
    "        self.vocab_size = None          # Nombre total de mots uniques dans le corpus\n",
    "        self.word_to_idx = None         # Dictionnaire: mot → index (ex: \"chat\" → 1)\n",
    "        self.idx_to_word = None         # Dictionnaire: index → mot (ex: 1 → \"chat\")\n",
    "        \n",
    "        # Poids du RNN (initialisés dans _initialize_weights)\n",
    "        self.W_x = None  # Matrice de poids: input → hidden state\n",
    "        self.W_h = None  # Matrice de poids: hidden state précédent → hidden state actuel\n",
    "        self.b_h = None  # Biais pour le hidden state\n",
    "        self.W_y = None  # Matrice de poids: hidden state → output (logits)\n",
    "        self.b_y = None  # Biais pour l'output\n",
    "    \n",
    "    def build_vocabulary(self,texts):\n",
    "        all_words = []\n",
    "        for text in texts:\n",
    "            words = text.split(\" \")\n",
    "            for word in words:\n",
    "                if word not in all_words:\n",
    "                    all_words.append(word)\n",
    "        \n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        \n",
    "        for i, word in enumerate(all_words):\n",
    "            self.word_to_idx.update({word:i})\n",
    "            self.idx_to_word.update({i:word})\n",
    "        \n",
    "        self.vocab_size = len(all_words)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialise les matrices de poids du RNN avec Xavier/Glorot\n",
    "        \n",
    "        Architecture du RNN:\n",
    "        - W_x : (hidden_size, vocab_size)    - Transforme l'input one-hot en hidden state\n",
    "        - W_h : (hidden_size, hidden_size)   - Propage l'information temporelle\n",
    "        - b_h : (hidden_size, 1)             - Biais du hidden state\n",
    "        - W_y : (vocab_size, hidden_size)    - Transforme hidden state en prédictions\n",
    "        - b_y : (vocab_size, 1)              - Biais de sortie\n",
    "        \"\"\"\n",
    "        # Initialisation Xavier/Glorot améliorée\n",
    "        # Scale adapté aux dimensions d'entrée/sortie pour éviter explosion/disparition des gradients\n",
    "        scale_x = np.sqrt(2.0 / (self.vocab_size + self.hidden_size))\n",
    "        scale_h = np.sqrt(2.0 / (self.hidden_size + self.hidden_size))\n",
    "        scale_y = np.sqrt(2.0 / (self.hidden_size + self.vocab_size))\n",
    "        \n",
    "        # W_x: Transforme le vecteur one-hot (vocab_size,) en hidden state (hidden_size,)\n",
    "        self.W_x = np.random.randn(self.hidden_size, self.vocab_size) * scale_x\n",
    "        \n",
    "        # W_h: Transforme le hidden state précédent (hidden_size,) en contribution au nouveau hidden state\n",
    "        self.W_h = np.random.randn(self.hidden_size, self.hidden_size) * scale_h\n",
    "        \n",
    "        # b_h: Biais ajouté au hidden state (permet un décalage de l'activation)\n",
    "        self.b_h = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # W_y: Transforme le hidden state (hidden_size,) en logits de prédiction (vocab_size,)\n",
    "        self.W_y = np.random.randn(self.vocab_size, self.hidden_size) * scale_y\n",
    "        \n",
    "        # b_y: Biais ajouté aux logits de sortie\n",
    "        self.b_y = np.zeros((self.vocab_size, 1))\n",
    "    \n",
    "    def word_to_vector(self, word):\n",
    "        \"\"\"Convertit un mot en vecteur one-hot\n",
    "        \n",
    "        Args:\n",
    "            word: Mot à convertir (ex: \"chat\")\n",
    "            \n",
    "        Returns:\n",
    "            vector: Vecteur one-hot de taille (vocab_size, 1)\n",
    "                    avec un 1 à la position du mot, 0 ailleurs\n",
    "                    Ex: si \"chat\" a l'index 1 dans un vocab de 5 mots:\n",
    "                    [[0], [1], [0], [0], [0]]\n",
    "        \"\"\"\n",
    "        idx = self.word_to_idx[word]\n",
    "        vector = np.zeros((self.vocab_size, 1))\n",
    "        vector[idx] = 1\n",
    "        \n",
    "        return vector\n",
    "    \n",
    "    def forward_step(self, x_t, h_prev):\n",
    "        \"\"\"Calcule un pas de temps forward du RNN\n",
    "        \n",
    "        Formule: h_t = tanh(W_h @ h_prev + W_x @ x_t + b_h)\n",
    "        \n",
    "        Args:\n",
    "            x_t: Input au temps t (vecteur one-hot du mot), shape (vocab_size, 1)\n",
    "            h_prev: Hidden state précédent, shape (hidden_size, 1)\n",
    "            \n",
    "        Returns:\n",
    "            h_t: Nouveau hidden state, shape (hidden_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        term1 = np.dot(self.W_h, h_prev)  # Contribution du passé\n",
    "        term2 = np.dot(self.W_x, x_t)     # Contribution de l'input actuel\n",
    "        z = term1 + term2 + self.b_h      # Combinaison linéaire\n",
    "        \n",
    "        h_t = np.tanh(z)  # Activation non-linéaire (tanh entre -1 et 1)\n",
    "        \n",
    "        return h_t\n",
    "    \n",
    "    def forward_sequence(self, sentence):\n",
    "        \"\"\"Calcule les hidden states pour tous les mots d'une phrase\n",
    "        \n",
    "        Args:\n",
    "            sentence: Phrase d'entrée (string)\n",
    "            \n",
    "        Returns:\n",
    "            states: Liste des hidden states, un par mot\n",
    "        \"\"\"\n",
    "        words = sentence.split(\" \")\n",
    "        h = np.zeros((self.hidden_size, 1))  # Hidden state initial (vecteur de zéros)\n",
    "        \n",
    "        states = []\n",
    "        \n",
    "        for word in words:\n",
    "            x_t = self.word_to_vector(word)  # One-hot encoding du mot actuel\n",
    "            h = self.forward_step(x_t, h)    # Calcul du nouveau hidden state\n",
    "            states.append(h)                  # Sauvegarde pour backprop\n",
    "        \n",
    "        return states\n",
    "    \n",
    "    def predict_word(self, h_t):\n",
    "        \"\"\"Prédit les probabilités de chaque mot à partir d'un hidden state\n",
    "        \n",
    "        Formule: \n",
    "            logits = W_y @ h_t + b_y\n",
    "            probabilities = softmax(logits)\n",
    "        \n",
    "        Args:\n",
    "            h_t: Hidden state actuel, shape (hidden_size, 1)\n",
    "            \n",
    "        Returns:\n",
    "            probabilities: Vecteur de probabilités pour chaque mot, shape (vocab_size, 1)\n",
    "        \"\"\"\n",
    "    \n",
    "        logits = np.dot(self.W_y, h_t) + self.b_y  # Scores bruts pour chaque mot\n",
    "        \n",
    "        # Softmax avec stabilité numérique\n",
    "        exp_logits = np.exp(logits - np.max(logits))  # Soustraction du max pour stabilité\n",
    "        sum_exp = np.sum(exp_logits)\n",
    "        probabilities = exp_logits / sum_exp  # Normalisation en probabilités (somme = 1)\n",
    "    \n",
    "        return probabilities\n",
    "    \n",
    "    def predict_next_word(self, context, top_k=10):\n",
    "        \"\"\"Prédit le mot suivant après un contexte donné\n",
    "        \n",
    "        Args:\n",
    "            context: Phrase de contexte\n",
    "            top_k: Nombre de mots les plus probables à afficher (défaut: 10)\n",
    "        \"\"\"\n",
    "        states = self.forward_sequence(context)\n",
    "        last_state = states[-1]  # État après le dernier mot\n",
    "    \n",
    "        probabilities = self.predict_word(last_state)\n",
    "    \n",
    "        # Trouver les top K mots les plus probables\n",
    "        probs_flat = probabilities.flatten()\n",
    "        top_indices = np.argsort(probs_flat)[::-1][:top_k]\n",
    "        \n",
    "        print(f\"Après '{context}', top {top_k} prédictions :\")\n",
    "        for rank, idx in enumerate(top_indices, 1):\n",
    "            word = self.idx_to_word[idx]\n",
    "            prob = probs_flat[idx]\n",
    "            print(f\"  {rank}. {word:15s} -> {prob:.4f}\")\n",
    "    \n",
    "        # Mot le plus probable\n",
    "        best_idx = top_indices[0]\n",
    "        best_word = self.idx_to_word[best_idx]\n",
    "        best_prob = probs_flat[best_idx]\n",
    "    \n",
    "        print(f\"\\nMot le plus probable: '{best_word}' (prob: {best_prob:.4f})\")\n",
    "        return best_word\n",
    "    \n",
    "    def train_on_sentence(self, sentence, learning_rate):\n",
    "        \"\"\"Entraîne le RNN sur une phrase avec backpropagation through time (BPTT)\n",
    "        \n",
    "        Args:\n",
    "            sentence: Phrase d'entraînement\n",
    "            learning_rate: Taux d'apprentissage (step size pour la descente de gradient)\n",
    "            \n",
    "        Returns:\n",
    "            total_loss: Loss totale sur la phrase\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward pass: calcul de tous les hidden states\n",
    "        states = self.forward_sequence(sentence)\n",
    "        \n",
    "        # Initialisation des gradients (accumulateurs)\n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        dW_x = np.zeros_like(self.W_x)\n",
    "        dW_h = np.zeros_like(self.W_h)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "\n",
    "        words = sentence.split(\" \")\n",
    "        total_loss = 0\n",
    "\n",
    "        # Gradient du hidden state venant du futur (pour BPTT)\n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Backpropagation à travers le temps (de la fin vers le début)\n",
    "        for i in range(len(words) - 1, 0, -1):\n",
    "            h_t = states[i-1]  # État après le mot i-1 (pour prédire le mot i)\n",
    "            target_word = words[i]\n",
    "\n",
    "            probabilities = self.predict_word(h_t)\n",
    "\n",
    "            # Calcul de la loss (cross-entropy)\n",
    "            target_index = self.word_to_idx[target_word]\n",
    "            target_probability = probabilities[target_index]\n",
    "            loss = -np.log(target_probability + 1e-8)  # Éviter log(0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Gradient de la loss par rapport à la sortie softmax\n",
    "            grad_output = probabilities.copy()\n",
    "            grad_output[target_index] -= 1  # Dérivée de cross-entropy + softmax\n",
    "\n",
    "            # Gradients de W_y et b_y\n",
    "            dW_y += grad_output @ h_t.T\n",
    "            db_y += grad_output\n",
    "\n",
    "            # Gradient backpropagé vers le hidden state\n",
    "            # Combine le gradient de la sortie ET celui du pas de temps suivant\n",
    "            grad_h_t = self.W_y.T @ grad_output + dh_next\n",
    "\n",
    "            # Obtenir l'état précédent et l'input\n",
    "            if i == 1:\n",
    "                h_prev = np.zeros((self.hidden_size, 1))\n",
    "            else:\n",
    "                h_prev = states[i-2]\n",
    "            \n",
    "            x_t = self.word_to_vector(words[i-1])\n",
    "\n",
    "            # Gradient à travers tanh: dtanh(z)/dz = 1 - tanh²(z)\n",
    "            grad_tanh = 1 - h_t**2\n",
    "            grad_z = grad_h_t * grad_tanh\n",
    "\n",
    "            # Gradients de W_h, W_x et b_h\n",
    "            dW_h += grad_z @ h_prev.T\n",
    "            dW_x += grad_z @ x_t.T\n",
    "            db_h += grad_z\n",
    "\n",
    "            # Gradient à propager au pas de temps précédent\n",
    "            dh_next = self.W_h.T @ grad_z\n",
    "\n",
    "        # Clipping des gradients pour éviter l'explosion\n",
    "        max_norm = 5.0\n",
    "        for grad in [dW_y, db_y, dW_x, dW_h, db_h]:\n",
    "            norm = np.linalg.norm(grad)\n",
    "            if norm > max_norm:\n",
    "                grad *= max_norm / norm\n",
    "\n",
    "        # Mise à jour des poids (descente de gradient)\n",
    "        self.W_y -= learning_rate * dW_y\n",
    "        self.b_y -= learning_rate * db_y\n",
    "        self.W_x -= learning_rate * dW_x\n",
    "        self.W_h -= learning_rate * dW_h\n",
    "        self.b_h -= learning_rate * db_h\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19a5a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire créé avec 182 mots uniques\n",
      "Architecture du modèle: hidden_size=64\n",
      "\n",
      "=== Exemples de mots du vocabulaire ===\n",
      "le, chat, mange, du, poisson, frais, dort, sur, canapé, confortable, joue, avec, la, balle, rouge, chien, aboie, très, fort, dans\n",
      "\n",
      "=== Test Forward Pass ===\n",
      "Phrase: 'le chat mange du poisson frais'\n",
      "Nombre d'états calculés: 6\n",
      "Dimension de chaque état caché: (64, 1)\n",
      "\n",
      "Somme des probabilités (doit être ~1.0): 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Données d'entraînement - Corpus enrichi avec phrases plus complexes\n",
    "texts = [\n",
    "    # --- Thème : Vie quotidienne & Actions ---\n",
    "    \"le chat mange du poisson frais\",\n",
    "    \"le chat dort sur le canapé confortable\",\n",
    "    \"le chat joue avec la balle rouge\",\n",
    "    \"le chien aboie très fort dans le jardin\",\n",
    "    \"le chien court rapidement dans le parc\",\n",
    "    \"le chien mange sa nourriture préférée\",\n",
    "    \n",
    "    # --- Thème : Cuisine & Gastronomie ---\n",
    "    \"le chef cuisine une délicieuse tarte aux pommes\",\n",
    "    \"le chef prépare un excellent plat de poisson\",\n",
    "    \"je mange une pomme rouge et juteuse\",\n",
    "    \"je bois un café noir très chaud\",\n",
    "    \"je prépare le diner pour ma famille\",\n",
    "    \"tu bois de l eau fraîche et pure\",\n",
    "    \"elle cuisine un gâteau au chocolat fondant\",\n",
    "    \"le gâteau est très sucré et délicieux\",\n",
    "    \"le four est extrêmement chaud et dangereux\",\n",
    "    \"il coupe le pain frais du matin\",\n",
    "    \n",
    "    # --- Thème : Sport & Compétition ---\n",
    "    \"le joueur court très vite sur le terrain\",\n",
    "    \"le joueur lance la balle avec précision\",\n",
    "    \"il lance la balle très haut dans le ciel\",\n",
    "    \"je cours rapidement dans le grand parc\",\n",
    "    \"tu nages vite dans la piscine olympique\",\n",
    "    \"le ballon est parfaitement rond et léger\",\n",
    "    \"le match est finalement fini après prolongation\",\n",
    "    \"l équipe gagne brillamment le match difficile\",\n",
    "    \"le sport est très bon pour la santé\",\n",
    "    \n",
    "    # --- Thème : Nature & Météo ---\n",
    "    \"le soleil brille intensément dans le ciel\",\n",
    "    \"le soleil est très chaud en été\",\n",
    "    \"la lune est magnifiquement blanche cette nuit\",\n",
    "    \"la lune brille doucement dans la nuit\",\n",
    "    \"le vent souffle très fort ce matin\",\n",
    "    \"les oiseaux volent très haut dans le ciel\",\n",
    "    \"les oiseaux chantent joliment dans les arbres\",\n",
    "    \"la mer est profondément bleue et calme\",\n",
    "    \"la forêt est magnifiquement verte au printemps\",\n",
    "    \"la forêt abrite de nombreux animaux sauvages\",\n",
    "    \n",
    "    # --- Thème : Descriptions & Observations ---\n",
    "    \"la table est très grande et solide\",\n",
    "    \"la maison est confortable et spacieuse\",\n",
    "    \"le livre est intéressant et captivant\",\n",
    "    \"le film est passionnant et émouvant\",\n",
    "    \"la musique est douce et relaxante\",\n",
    "    \"le professeur explique clairement la leçon difficile\",\n",
    "    \"l étudiant travaille sérieusement pour réussir son examen\",\n",
    "    \n",
    "    # --- Thème : Émotions & États ---\n",
    "    \"je suis très content de te voir\",\n",
    "    \"tu es vraiment fatigué après le travail\",\n",
    "    \"il est extrêmement heureux de sa réussite\",\n",
    "    \"elle est profondément triste de partir\",\n",
    "    \"nous sommes fiers de nos résultats\",\n",
    "    \n",
    "    # --- Phrases avec structures variées ---\n",
    "    \"quand le soleil brille je suis heureux\",\n",
    "    \"si tu cours vite tu gagneras la course\",\n",
    "    \"le chat et le chien jouent ensemble\",\n",
    "    \"je mange et je bois tranquillement\",\n",
    "    \"le chef cuisine pendant que les invités arrivent\"\n",
    "]\n",
    "\n",
    "# Création et initialisation du modèle avec hidden size AUGMENTÉ\n",
    "model = RNNLanguageModel(hidden_size=64)  # 64 au lieu de 20 pour plus de capacité\n",
    "model.build_vocabulary(texts)\n",
    "\n",
    "print(\"Vocabulaire créé avec\", model.vocab_size, \"mots uniques\")\n",
    "print(f\"Architecture du modèle: hidden_size={model.hidden_size}\")\n",
    "print(\"\\n=== Exemples de mots du vocabulaire ===\")\n",
    "sample_words = list(model.word_to_idx.keys())[:20]\n",
    "print(\", \".join(sample_words))\n",
    "\n",
    "# Test du forward pass\n",
    "print(\"\\n=== Test Forward Pass ===\")\n",
    "sentence = \"le chat mange du poisson frais\"\n",
    "states = model.forward_sequence(sentence)\n",
    "print(f\"Phrase: '{sentence}'\")\n",
    "print(f\"Nombre d'états calculés: {len(states)}\")\n",
    "print(f\"Dimension de chaque état caché: {states[0].shape}\")\n",
    "\n",
    "# Test de prédiction\n",
    "h_t = states[0]  # État après \"le\"\n",
    "probs = model.predict_word(h_t)\n",
    "print(f\"\\nSomme des probabilités (doit être ~1.0): {np.sum(probs):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e584f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AVANT entraînement ===\n",
      "Après 'je bois un', top 10 prédictions :\n",
      "  1. chat            -> 0.0078\n",
      "  2. sur             -> 0.0069\n",
      "  3. en              -> 0.0068\n",
      "  4. pain            -> 0.0067\n",
      "  5. magnifiquement  -> 0.0067\n",
      "  6. prépare         -> 0.0067\n",
      "  7. et              -> 0.0066\n",
      "  8. son             -> 0.0066\n",
      "  9. émouvant        -> 0.0065\n",
      "  10. parfaitement    -> 0.0065\n",
      "\n",
      "Mot le plus probable: 'chat' (prob: 0.0078)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chat'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du modèle non entraîné\n",
    "print(\"=== AVANT entraînement ===\")\n",
    "model.predict_next_word(\"je bois un\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92cs1keg0hf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ENTRAÎNEMENT DU MODÈLE ===\n",
      "Epoch 0/300: Loss moyenne = 30.5521\n",
      "Epoch 30/300: Loss moyenne = 5.1141\n",
      "Epoch 60/300: Loss moyenne = 3.1900\n",
      "Epoch 90/300: Loss moyenne = 2.8003\n",
      "Epoch 120/300: Loss moyenne = 2.7187\n",
      "Epoch 150/300: Loss moyenne = 2.6714\n",
      "Epoch 180/300: Loss moyenne = 2.6319\n",
      "Epoch 210/300: Loss moyenne = 2.6051\n",
      "Epoch 240/300: Loss moyenne = 2.5840\n",
      "Epoch 270/300: Loss moyenne = 2.5657\n",
      "Epoch 300/300: Loss moyenne = 2.5500\n",
      "\n",
      "Entraînement terminé sur 52 phrases!\n",
      "Loss finale: 2.5500 (objectif: < 5.0 pour de bonnes prédictions)\n",
      "\n",
      "======================================================================\n",
      "=== DÉMONSTRATION DE LA PUISSANCE DU MODÈLE ===\n",
      "======================================================================\n",
      "\n",
      "[Test 1] Contexte simple - 'le chat'\n",
      "Attendu: 'mange', 'dort', ou 'joue' (tous présents dans le corpus)\n",
      "Après 'le chat', top 5 prédictions :\n",
      "  1. et              -> 0.3940\n",
      "  2. joue            -> 0.2422\n",
      "  3. dort            -> 0.1956\n",
      "  4. mange           -> 0.1524\n",
      "  5. chef            -> 0.0026\n",
      "\n",
      "Mot le plus probable: 'et' (prob: 0.3940)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[Test 2] Contexte enrichi - 'le chat mange du poisson'\n",
      "Attendu: 'frais' (phrase exacte dans le corpus)\n",
      "Après 'le chat mange du poisson', top 5 prédictions :\n",
      "  1. frais           -> 0.9969\n",
      "  2. est             -> 0.0010\n",
      "  3. canapé          -> 0.0005\n",
      "  4. bois            -> 0.0004\n",
      "  5. jouent          -> 0.0003\n",
      "\n",
      "Mot le plus probable: 'frais' (prob: 0.9969)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[Test 3] Avec adverbe - 'le joueur court très'\n",
      "Attendu: 'vite' (de 'le joueur court très vite sur le terrain')\n",
      "Après 'le joueur court très', top 5 prédictions :\n",
      "  1. vite            -> 0.9969\n",
      "  2. mange           -> 0.0010\n",
      "  3. et              -> 0.0006\n",
      "  4. balle           -> 0.0006\n",
      "  5. fort            -> 0.0005\n",
      "\n",
      "Mot le plus probable: 'vite' (prob: 0.9969)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[Test 4] Pattern 'est très' - 'le gâteau est très'\n",
      "Attendu: 'sucré' ou autres adjectifs\n",
      "Après 'le gâteau est très', top 5 prédictions :\n",
      "  1. sucré           -> 0.9951\n",
      "  2. bon             -> 0.0010\n",
      "  3. chaud           -> 0.0007\n",
      "  4. haut            -> 0.0007\n",
      "  5. rond            -> 0.0005\n",
      "\n",
      "Mot le plus probable: 'sucré' (prob: 0.9951)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[Test 5] Contexte naturel - 'le soleil brille'\n",
      "Attendu: 'intensément' ou 'doucement'\n",
      "Après 'le soleil brille', top 5 prédictions :\n",
      "  1. intensément     -> 0.9906\n",
      "  2. doucement       -> 0.0027\n",
      "  3. pendant         -> 0.0024\n",
      "  4. confortable     -> 0.0007\n",
      "  5. rapidement      -> 0.0006\n",
      "\n",
      "Mot le plus probable: 'intensément' (prob: 0.9906)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[Test 6] Contexte avec préposition - 'les oiseaux volent très haut dans'\n",
      "Attendu: 'le' (de 'dans le ciel')\n",
      "Après 'les oiseaux volent très haut dans', top 5 prédictions :\n",
      "  1. le              -> 0.9999\n",
      "  2. la              -> 0.0001\n",
      "  3. arbres          -> 0.0000\n",
      "  4. les             -> 0.0000\n",
      "  5. tu              -> 0.0000\n",
      "\n",
      "Mot le plus probable: 'le' (prob: 0.9999)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[Test 7] Coordination - 'le chat et le chien'\n",
      "Attendu: 'jouent' (de 'le chat et le chien jouent ensemble')\n",
      "Après 'le chat et le chien', top 5 prédictions :\n",
      "  1. jouent          -> 0.9916\n",
      "  2. frais           -> 0.0015\n",
      "  3. canapé          -> 0.0015\n",
      "  4. plat            -> 0.0009\n",
      "  5. mange           -> 0.0008\n",
      "\n",
      "Mot le plus probable: 'jouent' (prob: 0.9916)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[Test 8] Phrase complexe - 'je suis très content de'\n",
      "Attendu: 'te' (de 'je suis très content de te voir')\n",
      "Après 'je suis très content de', top 5 prédictions :\n",
      "  1. te              -> 0.9965\n",
      "  2. grand           -> 0.0008\n",
      "  3. partir          -> 0.0004\n",
      "  4. course          -> 0.0002\n",
      "  5. réussir         -> 0.0002\n",
      "\n",
      "Mot le plus probable: 'te' (prob: 0.9965)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "✓ Tests terminés!\n",
      "Le modèle (hidden_size=64) a appris sur 300 epochs\n",
      "avec un learning rate de 0.05\n"
     ]
    }
   ],
   "source": [
    "# ENTRAÎNEMENT AMÉLIORÉ - Sur TOUTES les phrases du corpus\n",
    "print(\"\\n=== ENTRAÎNEMENT DU MODÈLE ===\")\n",
    "learning_rate = 0.05  # Learning rate augmenté (était 0.01)\n",
    "epochs = 300  # Plus d'epochs pour mieux apprendre\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_loss = 0\n",
    "    \n",
    "    # Entraîner sur TOUTES les phrases\n",
    "    for sentence in texts:\n",
    "        loss = model.train_on_sentence(sentence, learning_rate)\n",
    "        total_epoch_loss += loss\n",
    "    \n",
    "    # Afficher la progression\n",
    "    if epoch % 30 == 0:\n",
    "        avg_loss = total_epoch_loss / len(texts)\n",
    "        print(f\"Epoch {epoch}/{epochs}: Loss moyenne = {avg_loss:.4f}\")\n",
    "\n",
    "# Dernière loss\n",
    "avg_loss = total_epoch_loss / len(texts)\n",
    "print(f\"Epoch {epochs}/{epochs}: Loss moyenne = {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nEntraînement terminé sur {len(texts)} phrases!\")\n",
    "print(f\"Loss finale: {avg_loss:.4f} (objectif: < 5.0 pour de bonnes prédictions)\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=== DÉMONSTRATION DE LA PUISSANCE DU MODÈLE ===\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 1: Contexte simple\n",
    "print(\"\\n[Test 1] Contexte simple - 'le chat'\")\n",
    "print(\"Attendu: 'mange', 'dort', ou 'joue' (tous présents dans le corpus)\")\n",
    "model.predict_next_word(\"le chat\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Test 2: Contexte plus long avec adjectifs\n",
    "print(\"\\n[Test 2] Contexte enrichi - 'le chat mange du poisson'\")\n",
    "print(\"Attendu: 'frais' (phrase exacte dans le corpus)\")\n",
    "model.predict_next_word(\"le chat mange du poisson\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Test 3: Structure sujet-verbe avec adverbes\n",
    "print(\"\\n[Test 3] Avec adverbe - 'le joueur court très'\")\n",
    "print(\"Attendu: 'vite' (de 'le joueur court très vite sur le terrain')\")\n",
    "model.predict_next_word(\"le joueur court très\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Test 4: Patterns répétitifs (est + adverbe)\n",
    "print(\"\\n[Test 4] Pattern 'est très' - 'le gâteau est très'\")\n",
    "print(\"Attendu: 'sucré' ou autres adjectifs\")\n",
    "model.predict_next_word(\"le gâteau est très\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Test 5: Contexte météo\n",
    "print(\"\\n[Test 5] Contexte naturel - 'le soleil brille'\")\n",
    "print(\"Attendu: 'intensément' ou 'doucement'\")\n",
    "model.predict_next_word(\"le soleil brille\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Test 6: Structure complexe avec prépositions\n",
    "print(\"\\n[Test 6] Contexte avec préposition - 'les oiseaux volent très haut dans'\")\n",
    "print(\"Attendu: 'le' (de 'dans le ciel')\")\n",
    "model.predict_next_word(\"les oiseaux volent très haut dans\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Test 7: Coordination\n",
    "print(\"\\n[Test 7] Coordination - 'le chat et le chien'\")\n",
    "print(\"Attendu: 'jouent' (de 'le chat et le chien jouent ensemble')\")\n",
    "model.predict_next_word(\"le chat et le chien\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Test 8: Contexte émotionnel long\n",
    "print(\"\\n[Test 8] Phrase complexe - 'je suis très content de'\")\n",
    "print(\"Attendu: 'te' (de 'je suis très content de te voir')\")\n",
    "model.predict_next_word(\"je suis très content de\", top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n✓ Tests terminés!\")\n",
    "print(f\"Le modèle (hidden_size={model.hidden_size}) a appris sur {epochs} epochs\")\n",
    "print(f\"avec un learning rate de {learning_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
